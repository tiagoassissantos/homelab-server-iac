---
- name: Configure Alpine Linux k3s Server
  hosts: masters
  become: true
  gather_facts: true
  vars:
    ansible_python_interpreter: /usr/bin/python3

  tasks:
    # === System Prerequisites ===
    - name: Update apk cache
      apk:
        update_cache: yes

    - name: Install base packages
      apk:
        name:
          - curl
          - wget
          - git
          - python3
          - py3-pip
          - py3-yaml
          - py3-kubernetes
          - py3-cryptography
          - py3-setuptools
          - openssl
          - ca-certificates
          - nftables
          - cni-plugins
          - containerd
          - runc
          - zsh
          - shadow
          - bash
        state: present

    # === Configure zsh and oh-my-zsh ===
    - name: Change shell to zsh for user
      user:
        name: "{{ ansible_user }}"
        shell: /bin/zsh

    - name: Install oh-my-zsh
      shell: |
        if [ ! -d "/home/{{ ansible_user }}/.oh-my-zsh" ]; then
          su - {{ ansible_user }} -c 'sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)" "" --unattended'
        fi
      args:
        creates: "/home/{{ ansible_user }}/.oh-my-zsh"

    - name: Set zsh theme to robbyrussell
      lineinfile:
        path: "/home/{{ ansible_user }}/.zshrc"
        regexp: "^ZSH_THEME="
        line: 'ZSH_THEME="robbyrussell"'
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    # === Kernel Modules for k3s ===
    - name: Load required kernel modules
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter
        - nf_conntrack
        - nf_nat
        - nf_tables
        - nfnetlink
        - xt_REDIRECT
        - xt_owner

    - name: Ensure kernel modules load on boot
      lineinfile:
        path: /etc/modules-load.d/k3s.conf
        line: "{{ item }}"
        create: yes
      loop:
        - overlay
        - br_netfilter
        - nf_conntrack
        - nf_nat
        - nf_tables
        - nfnetlink
        - xt_REDIRECT
        - xt_owner

    # === Sysctl settings ===
    - name: Configure sysctl for k3s
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        sysctl_set: yes
        state: present
        reload: yes
      loop:
        - { name: "net.bridge.bridge-nf-call-iptables", value: "1" }
        - { name: "net.bridge.bridge-nf-call-ip6tables", value: "1" }
        - { name: "net.ipv4.ip_forward", value: "1" }
        - { name: "net.ipv6.conf.all.forwarding", value: "1" }
        - { name: "vm.overcommit_memory", value: "1" }
        - { name: "kernel.panic", value: "10" }
        - { name: "kernel.panic_on_oops", value: "1" }

    # === Configure Firewall ===
    - name: Ensure nftables package is present
      apk:
        name: nftables
        state: present
        update_cache: true

    - name: Ensure nftables include directory exists
      file:
        path: /etc/nftables.d
        state: directory
        mode: "0755"

    - name: Remove conflicting firewall packages
      apk:
        name:
          - iptables-legacy
          - awall
        state: absent
      ignore_errors: true

    - name: Remove old nftables.d directory to avoid conflicts
      file:
        path: /etc/nftables.d
        state: absent

    - name: Ensure iptables uses nft backend (Alpine)
      shell: |
        if command -v update-alternatives >/dev/null 2>&1; then
          if [ -e /sbin/iptables-nft ]; then update-alternatives --set iptables /sbin/iptables-nft || true; fi
          if [ -e /sbin/ip6tables-nft ]; then update-alternatives --set ip6tables /sbin/ip6tables-nft || true; fi
        fi

    - name: Stop nftables service before flushing
      service:
        name: nftables
        state: stopped
      ignore_errors: true

    - name: Flush all existing nftables rules
      tags: [flush_fw_rules]
      shell: |
        nft flush ruleset || true

    - name: Ensure /etc/nftables.nft includes /etc/nftables.d/*.nft
      lineinfile:
        path: /etc/nftables.nft
        regexp: '^include "/etc/nftables.d/\*\.nft";$'
        line: 'include "/etc/nftables.d/*.nft";'
        create: yes

    - name: Create complete nftables configuration for MetalLB compatibility
      copy:
        dest: /etc/nftables.nft
        mode: "0644"
        backup: yes
        content: |
          #!/usr/bin/nft -f

          # Flush all existing rules
          flush ruleset

          table inet filter {
            chain input {
              type filter hook input priority filter; policy drop;

              # Allow loopback traffic
              iifname "lo" accept comment "Accept any localhost traffic"

              # Allow established and related connections
              ct state established,related accept comment "Accept traffic originated from us"

              # Drop invalid connections
              ct state invalid drop comment "Drop invalid connections"

              # Allow ICMP/ICMPv6
              ip protocol icmp icmp type { echo-reply, destination-unreachable, echo-request, time-exceeded, parameter-problem } accept comment "Accept ICMP"
              icmpv6 type { destination-unreachable, packet-too-big, time-exceeded, parameter-problem, echo-request, echo-reply } accept comment "Accept basic IPv6 functionality"

              # Allow SSH
              tcp dport 22 accept comment "accept SSH"

              # DNS traffic (critical for Pi-hole)
              tcp dport { 53, 5353 } accept comment "DNS TCP"
              udp dport { 53, 5353 } accept comment "DNS UDP"

              # Kubernetes services
              tcp dport 6443 accept comment "Kubernetes API server"
              tcp dport 10250 accept comment "Kubelet API"
              tcp dport { 2379, 2380 } accept comment "etcd client and peer"
              tcp dport 10256 accept comment "kube-proxy health check"

              # MetalLB
              tcp dport { 7472, 9443 } accept comment "MetalLB controller and webhook"

              # Web services
              tcp dport { 80, 443 } accept comment "HTTP and HTTPS"

              # Flannel VXLAN
              udp dport 8472 accept comment "Flannel VXLAN"

              # NodePort range
              tcp dport 30000-32767 accept comment "NodePort TCP range"
              udp dport 30000-32767 accept comment "NodePort UDP range"

              # Allow Kubernetes networks
              ip saddr 10.42.0.0/16 accept comment "Pod network source"
              ip daddr 10.42.0.0/16 accept comment "Pod network destination"
              ip saddr 10.43.0.0/16 accept comment "Service network source"
              ip daddr 10.43.0.0/16 accept comment "Service network destination"

              # Allow local network
              ip saddr 10.0.0.0/24 accept comment "Local network source"
              ip daddr 10.0.0.0/24 accept comment "Local network destination"

              # PostmarketOS specific rules
              iifname "wwan*" drop comment "drop all connections on wwan"
              iifname "usb*" udp dport 67 accept comment "accept incoming DHCP on usb*"
              iifname "wlan*" udp dport 67 accept comment "accept incoming DHCP on wlan*"
              iifname "p2p-wlan*" udp dport 67 accept comment "accept incoming DHCP on p2p-wlan*"
              iifname "usb*" accept comment "Allow incoming network traffic from USB"
              iifname "wlan*" tcp dport 53 accept comment "Accept DNS over TCP on wlan*"
              iifname "wlan*" udp dport 53 accept comment "Accept DNS over UDP on wlan*"

              drop
            }

            chain forward {
              type filter hook forward priority filter; policy accept;

              # Allow established connections
              ct state established,related accept comment "accept established connections"

              # Allow all Kubernetes traffic
              ip saddr 10.42.0.0/16 accept comment "Pod network forwarding"
              ip daddr 10.42.0.0/16 accept comment "Pod network forwarding"
              ip saddr 10.43.0.0/16 accept comment "Service network forwarding"
              ip daddr 10.43.0.0/16 accept comment "Service network forwarding"

              # Allow container interface traffic
              iifname { "cni*", "flannel*", "veth*", "docker*", "br-*" } accept comment "Container interfaces"
              oifname { "cni*", "flannel*", "veth*", "docker*", "br-*" } accept comment "Container interfaces"

              # PostmarketOS specific forwarding
              iifname "wlan*" accept comment "Accept forwarding from wlan*"
              iifname "usb*" accept comment "Allow outgoing network traffic from USB"

              # Allow Flannel VXLAN forwarding
              udp dport 8472 accept comment "Flannel VXLAN forwarding"

              accept
            }

            chain output {
              type filter hook output priority filter; policy accept;
              accept
            }
          }

          table inet nat {
            chain prerouting {
              type nat hook prerouting priority dstnat; policy accept;
              # Let kube-proxy handle service routing
            }

            chain postrouting {
              type nat hook postrouting priority srcnat; policy accept;
              # Essential NAT for pod traffic going external
              ip saddr 10.42.0.0/16 oifname != { "cni*", "flannel*", "veth*", "docker*", "br-*" } masquerade comment "Pod to external NAT"
            }
          }

    - name: Enable and start nftables with new configuration
      tags: [enable_fw_rules]
      service:
        name: nftables
        enabled: yes
        state: started

    - name: Wait for nftables service to be ready
      wait_for:
        timeout: 15

    - name: Verify K3s nftables rules are loaded
      shell: |
        nft list ruleset | grep -q "inet filter" && echo "SUCCESS: K3s inet filter rules loaded"
      register: nft_verification
      retries: 3
      delay: 5
      until: nft_verification.rc == 0

    - name: Display verification result
      debug:
        msg: "{{ nft_verification.stdout }}"

    # === Install k3s ===
    - name: Download k3s install script
      get_url:
        url: https://get.k3s.io
        dest: /tmp/k3s-install.sh
        mode: "0755"

    - name: Install k3s server
      shell: |
        INSTALL_K3S_VERSION="{{ k3s_version }}" \
        INSTALL_K3S_CHANNEL="{{ k3s_channel }}" \
        sh /tmp/k3s-install.sh server {{ k3s_install_extra_args }} \
        --kube-apiserver-arg="bind-address=0.0.0.0" \
        --kube-apiserver-arg="advertise-address={{ ansible_default_ipv4.address }}"
      args:
        creates: /usr/local/bin/k3s

    - name: Wait for k3s to be ready
      wait_for:
        port: 6443
        host: localhost
        delay: 30
        timeout: 600

    - name: Test Kubernetes API connectivity from pod network
      shell: |
        kubectl run test-api-connectivity --image=alpine:latest --rm -i --restart=Never -- sh -c '
        apk add --no-cache curl >/dev/null 2>&1
        echo "Testing API connectivity from pod..."
        curl -k -m 10 https://kubernetes.default.svc.cluster.local/healthz || echo "API test failed"
        curl -k -m 10 https://10.43.0.1:443/healthz || echo "Service IP test failed"
        '
      ignore_errors: true
      register: api_test

    - name: Display API connectivity test results
      debug:
        var: api_test.stdout_lines
      when: api_test.stdout_lines is defined

    - name: Create .kube directory
      file:
        path: "/home/{{ ansible_user }}/.kube"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: "0755"

    - name: Copy k3s kubeconfig
      copy:
        src: /etc/rancher/k3s/k3s.yaml
        dest: "/home/{{ ansible_user }}/.kube/config"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: "0600"
        remote_src: yes

    # === Install Helm ===
    - name: Download Helm install script
      get_url:
        url: https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        dest: /tmp/get-helm-3.sh
        mode: "0755"

    - name: Install Helm
      shell: /tmp/get-helm-3.sh
      args:
        creates: /usr/local/bin/helm

    # === Install MetalLB ===
    - name: Add MetalLB Helm repository
      kubernetes.core.helm_repository:
        name: metallb
        repo_url: https://metallb.github.io/metallb
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"

    - name: Install MetalLB
      kubernetes.core.helm:
        name: metallb
        chart_ref: metallb/metallb
        release_namespace: metallb-system
        create_namespace: true
        chart_version: "{{ metallb_version }}"
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"

    - name: Wait for MetalLB controller
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        name: metallb-controller
        namespace: metallb-system
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        wait: true
        wait_condition:
          type: Available
          status: "True"
        wait_timeout: 300

    - name: Fix metrics-server configuration for connectivity issues
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: metrics-server
            namespace: kube-system
          spec:
            template:
              spec:
                containers:
                  - name: metrics-server
                    args:
                      - --cert-dir=/tmp
                      - --secure-port=10250
                      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
                      - --kubelet-use-node-status-port
                      - --metric-resolution=15s
                      - --kubelet-insecure-tls
        merge_type: strategic-merge # Only options available are "strategic-merge" and "merge"

    - name: Restart metrics-server pod to apply new configuration
      shell: |
        kubectl rollout restart deployment metrics-server -n kube-system
        kubectl rollout status deployment metrics-server -n kube-system --timeout=120s
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"

    - name: Wait for metrics-server to be ready with new configuration
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        name: metrics-server
        namespace: kube-system
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        wait: true
        wait_condition:
          type: Available
          status: "True"
        wait_timeout: 300
      ignore_errors: true

    - name: Create MetalLB IP address pool
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: metallb.io/v1beta1
          kind: IPAddressPool
          metadata:
            name: default-pool
            namespace: metallb-system
          spec:
            addresses:
              - "{{ metallb_address_pool }}"

    - name: Create MetalLB L2Advertisement
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: metallb.io/v1beta1
          kind: L2Advertisement
          metadata:
            name: default-advertise
            namespace: metallb-system
          spec:
            ipAddressPools:
              - default-pool

    # === Install cert-manager ===
    - name: Add cert-manager Helm repository
      kubernetes.core.helm_repository:
        name: jetstack
        repo_url: https://charts.jetstack.io
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"

    - name: Install cert-manager
      kubernetes.core.helm:
        name: cert-manager
        chart_ref: jetstack/cert-manager
        release_namespace: cert-manager
        create_namespace: true
        values:
          installCRDs: true
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"

    - name: Create self-signed ClusterIssuer
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: selfsigned-issuer
          spec:
            selfSigned: {}
      when: tls_mode == "selfsigned"

    # === Install Pi-hole ===
    - name: Create Pi-hole namespace
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        name: "{{ pihole_namespace }}"
        api_version: v1
        kind: Namespace
        state: present

    - name: Create Pi-hole ConfigMap
      tags: pihone_configmap
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: pihole-config
            namespace: "{{ pihole_namespace }}"
          data:
            DNS1: "{{ pihole_dns1 }}"
            DNS2: "{{ pihole_dns2 }}"

    - name: Create Pi-hole Secret
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: pihole-secret
            namespace: "{{ pihole_namespace }}"
          type: Opaque
          stringData:
            password: "{{ pihole_password }}"

    - name: Deploy Pi-hole
      tags: deploy_pihole
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: pihole
            namespace: "{{ pihole_namespace }}"
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: pihole
            template:
              metadata:
                labels:
                  app: pihole
              spec:
                containers:
                  - name: pihole
                    image: pihole/pihole:latest
                    env:
                      - name: WEBPASSWORD
                        valueFrom:
                          secretKeyRef:
                            name: pihole-secret
                            key: password
                      - name: DNS1
                        valueFrom:
                          configMapKeyRef:
                            name: pihole-config
                            key: DNS1
                      - name: DNS2
                        valueFrom:
                          configMapKeyRef:
                            name: pihole-config
                            key: DNS2
                      - name: TZ
                        value: "UTC"
                    ports:
                      - containerPort: 80
                        name: http
                      - containerPort: 53
                        name: dns-tcp
                        protocol: TCP
                      - containerPort: 53
                        name: dns-udp
                        protocol: UDP
                    volumeMounts:
                      - name: pihole-data
                        mountPath: /etc/pihole
                      - name: dnsmasq-data
                        mountPath: /etc/dnsmasq.d
                volumes:
                  - name: pihole-data
                    emptyDir: {}
                  - name: dnsmasq-data
                    emptyDir: {}

    - name: Create Pi-hole Service with proper traffic policy
      tags: [pihole_service]
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: pihole-service
            namespace: "{{ pihole_namespace }}"
          spec:
            type: LoadBalancer
            loadBalancerIP: "{{ pihole_lb_ip }}"
            externalTrafficPolicy: Cluster
            ports:
              - port: 80
                targetPort: 80
                name: http
              - name: dns-tcp
                port: 53
                targetPort: 53
                protocol: TCP
              - name: dns-udp
                port: 53
                targetPort: 53
                protocol: UDP
            selector:
              app: pihole

    # === Configure k3s to use Pi-hole as DNS ===
    - name: Configure k3s to use Pi-hole DNS
      lineinfile:
        path: /etc/systemd/resolved.conf
        regexp: "^DNS="
        line: "DNS={{ pihole_lb_ip }}"
        create: yes
      notify: restart systemd-resolved

    - name: Configure coredns to use Pi-hole
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: coredns
            namespace: kube-system
          data:
            Corefile: |
              .:53 {
                  errors
                  health {
                      lameduck 5s
                  }
                  ready
                  kubernetes cluster.local in-addr.arpa ip6.arpa {
                      pods insecure
                      fallthrough in-addr.arpa ip6.arpa
                      ttl 30
                  }
                  prometheus :9153
                  forward . {{ pihole_lb_ip }}:{{ pihole_dns_port }}
                  cache 30
                  loop
                  reload
                  loadbalance
              }

    # === Install Argo Workflows ===
    - name: Create Argo namespace
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        name: "{{ argo_namespace }}"
        api_version: v1
        kind: Namespace
        state: present

    - name: Add Argo Helm repository
      kubernetes.core.helm_repository:
        name: argo
        repo_url: https://argoproj.github.io/argo-helm
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"

    - name: Install Argo Workflows
      tags: install_argo_workflows
      kubernetes.core.helm:
        name: argo-workflows
        chart_ref: argo/argo-workflows
        release_namespace: "{{ argo_namespace }}"
        chart_version: "{{ argo_chart_version }}"
        values:
          server:
            serviceType: LoadBalancer
            servicePort: 80
            loadBalancerIP: "{{ argo_lb_ip }}"
            ingress:
              enabled: true
              hosts:
                - "{{ argo_host }}"
              paths:
                - /
              pathType: Prefix
              tls:
                - secretName: argo-tls
                  hosts:
                    - "{{ argo_host }}"
          workflow:
            serviceAccount:
              create: true
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"

    - name: Create Argo Workflows TLS certificate
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: argo-tls
            namespace: "{{ argo_namespace }}"
          spec:
            secretName: argo-tls
            issuerRef:
              name: selfsigned-issuer
              kind: ClusterIssuer
            commonName: "{{ argo_host }}"
            dnsNames:
              - "{{ argo_host }}"

    # === Create Argo Workflows Access Token ===
    - name: Create Argo admin service account
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: argo-admin
            namespace: "{{ argo_namespace }}"

    - name: Create Argo admin role
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: Role
          metadata:
            name: argo-admin
            namespace: "{{ argo_namespace }}"
          rules:
            - apiGroups: ["argoproj.io"]
              resources:
                [
                  "workflows",
                  "workflowtemplates",
                  "cronworkflows",
                  "clusterworkflowtemplates",
                ]
              verbs:
                ["list", "get", "create", "update", "delete", "patch", "watch"]
            - apiGroups: [""]
              resources: ["pods", "pods/log"]
              verbs: ["list", "get", "watch"]

    - name: Create Argo admin role binding
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: RoleBinding
          metadata:
            name: argo-admin
            namespace: "{{ argo_namespace }}"
          subjects:
            - kind: ServiceAccount
              name: argo-admin
              namespace: "{{ argo_namespace }}"
          roleRef:
            kind: Role
            name: argo-admin
            apiGroup: rbac.authorization.k8s.io

    - name: Create Argo admin token secret
      kubernetes.core.k8s:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: argo-admin.service-account-token
            namespace: "{{ argo_namespace }}"
            annotations:
              kubernetes.io/service-account.name: argo-admin
          type: kubernetes.io/service-account-token

    - name: Wait for token to be generated
      wait_for:
        timeout: 10

    - name: Get Argo access token
      kubernetes.core.k8s_info:
        kubeconfig: "/home/{{ ansible_user }}/.kube/config"
        api_version: v1
        kind: Secret
        name: argo-admin.service-account-token
        namespace: "{{ argo_namespace }}"
      register: argo_token_secret

    - name: Display Argo access token
      debug:
        msg: "Argo Token: Bearer {{ argo_token_secret.resources[0].data.token | b64decode }}"
      when: argo_token_secret.resources.data.token is defined

  handlers:
    - name: restart systemd-resolved
      service:
        name: systemd-resolved
        state: restarted
      ignore_errors: true
